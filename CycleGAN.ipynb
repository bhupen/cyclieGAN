{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "5Jbwk3FVpK9W",
    "outputId": "2c7b8f0d-f80c-4402-c69d-0455947b221c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fastai'...\n",
      "remote: Enumerating objects: 32655, done.\u001b[K\n",
      "remote: Total 32655 (delta 0), reused 0 (delta 0), pack-reused 32655\u001b[K\n",
      "Receiving objects: 100% (32655/32655), 439.73 MiB | 29.46 MiB/s, done.\n",
      "Resolving deltas: 100% (23840/23840), done.\n",
      "Checking out files: 100% (818/818), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/fastai/fastai.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yZdfQvwfn_II"
   },
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YCp8VfuQqdAr",
    "outputId": "f77516b7-4ce1-4578-f8af-e6585b54dec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/fastai/courses/dl2\n"
     ]
    }
   ],
   "source": [
    "cd /content/fastai/courses/dl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "x0Wy2Defved7",
    "outputId": "9e92b235-debb-4ec3-dbb8-c50039616be6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-28 14:27:21--  https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\n",
      "Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.189.73\n",
      "Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.189.73|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 116867962 (111M) [application/zip]\n",
      "Saving to: ‘horse2zebra.zip’\n",
      "\n",
      "horse2zebra.zip     100%[===================>] 111.45M  57.1MB/s    in 2.0s    \n",
      "\n",
      "2020-06-28 14:27:23 (57.1 MB/s) - ‘horse2zebra.zip’ saved [116867962/116867962]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYirb5-7us-G"
   },
   "outputs": [],
   "source": [
    "!unzip -q -n horse2zebra.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZvF4JMYXn_In"
   },
   "outputs": [],
   "source": [
    "from cgan.options.train_options import TrainOptions\n",
    "from cgan.data.data_loader import CreateDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "IhvHUm31_7AV",
    "outputId": "fc0615dc-36ad-4b69-de59-be5c5953675a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "batchSize: 1\n",
      "beta1: 0.5\n",
      "checkpoints_dir: ./checkpoints\n",
      "continue_train: False\n",
      "dataroot: horse2zebra\n",
      "dataset_mode: unaligned\n",
      "display_freq: 100\n",
      "display_id: 1\n",
      "display_port: 8097\n",
      "display_single_pane_ncols: 0\n",
      "display_winsize: 256\n",
      "epoch_count: 1\n",
      "fineSize: 256\n",
      "gpu_ids: [0]\n",
      "init_type: normal\n",
      "input_nc: 3\n",
      "isTrain: True\n",
      "lambda_A: 10.0\n",
      "lambda_B: 10.0\n",
      "lambda_identity: 0.5\n",
      "loadSize: 286\n",
      "lr: 0.0002\n",
      "lr_decay_iters: 50\n",
      "lr_policy: lambda\n",
      "max_dataset_size: inf\n",
      "model: cycle_gan\n",
      "nThreads: 8\n",
      "n_layers_D: 3\n",
      "name: nodrop\n",
      "ndf: 64\n",
      "ngf: 64\n",
      "niter: 100\n",
      "niter_decay: 100\n",
      "no_dropout: True\n",
      "no_flip: False\n",
      "no_html: False\n",
      "no_lsgan: False\n",
      "norm: instance\n",
      "output_nc: 3\n",
      "phase: train\n",
      "pool_size: 50\n",
      "print_freq: 100\n",
      "resize_or_crop: resize_and_crop\n",
      "save_epoch_freq: 5\n",
      "save_latest_freq: 5000\n",
      "serial_batches: False\n",
      "update_html_freq: 1000\n",
      "which_direction: AtoB\n",
      "which_epoch: latest\n",
      "which_model_netD: basic\n",
      "which_model_netG: resnet_9blocks\n",
      "-------------- End ----------------\n"
     ]
    }
   ],
   "source": [
    "opt = TrainOptions().parse(['--dataroot', 'horse2zebra', '--nThreads', '8', '--no_dropout',\n",
    "                           '--niter', '100', '--niter_decay', '100', '--name', 'nodrop', '--gpu_ids', '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Cq8AdTLFn_Iu",
    "outputId": "58e7a95d-18ed-4834-f427-46b8dcd38007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDatasetDataLoader\n",
      "dataset [UnalignedDataset] was created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1334"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = CreateDataLoader(opt)\n",
    "dataset = data_loader.load_data()\n",
    "dataset_size = len(data_loader)\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oxbRqPuyVpp"
   },
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48-ONrelzl7h"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuTuMPnO7sb8"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P54V2nid8Dgk"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zr4OBnQzcCs"
   },
   "source": [
    "## 2.1 Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Odm2COBqzUe6"
   },
   "outputs": [],
   "source": [
    "class BaseModel():\n",
    "    def name(self): return 'BaseModel'\n",
    "\n",
    "    def initialize(self, opt):\n",
    "        self.opt = opt\n",
    "        self.gpu_ids = opt.gpu_ids\n",
    "        self.isTrain = opt.isTrain\n",
    "        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n",
    "        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
    "\n",
    "    def set_input(self, input): self.input = input\n",
    "    def forward(self): pass\n",
    "    def test(self): pass\n",
    "    def get_image_paths(self): pass\n",
    "    def optimize_parameters(self): pass\n",
    "    def get_current_visuals(self): return self.input\n",
    "    def get_current_errors(self): return {}\n",
    "    def save(self, label): pass\n",
    "\n",
    "    # helper saving function that can be used by subclasses\n",
    "    def save_network(self, network, network_label, epoch_label, gpu_ids):\n",
    "        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "        save_path = os.path.join(self.save_dir, save_filename)\n",
    "        torch.save(network.cpu().state_dict(), save_path)\n",
    "        if len(gpu_ids) and torch.cuda.is_available(): network.cuda(gpu_ids[0])\n",
    "\n",
    "    # helper loading function that can be used by subclasses\n",
    "    def load_network(self, network, network_label, epoch_label):\n",
    "        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "        save_path = os.path.join(self.save_dir, save_filename)\n",
    "        network.load_state_dict(torch.load(save_path))\n",
    "\n",
    "    # update learning rate (called once every epoch)\n",
    "    def update_learning_rate(self):\n",
    "        for scheduler in self.schedulers: scheduler.step()\n",
    "        lr = self.optimizers[0].param_groups[0]['lr']\n",
    "        print('learning rate = %.7f' % lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VGt-PEW7CJs"
   },
   "source": [
    "## 2.2 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zd_fEVx7G32"
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.normal(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.normal(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        init.normal(m.weight.data, 1.0, 0.02)\n",
    "        init.constant(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def init_weights(net, init_type='normal'):\n",
    "    print('initialization method [%s]' % init_type)\n",
    "    if init_type == 'normal':\n",
    "        net.apply(weights_init_normal)\n",
    "    else:\n",
    "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "\n",
    "\n",
    "def get_norm_layer(norm_type='instance'):\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n",
    "    elif norm_type == 'none':\n",
    "        norm_layer = None\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, opt):\n",
    "    if opt.lr_policy == 'lambda':\n",
    "        def lambda_rule(epoch):\n",
    "            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n",
    "            return lr_l\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "    elif opt.lr_policy == 'step':\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n",
    "    elif opt.lr_policy == 'plateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
    "    else:\n",
    "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def define_G(input_nc, output_nc, ngf, which_model_netG, norm='batch', use_dropout=False, init_type='normal', gpu_ids=[]):\n",
    "    netG = None\n",
    "    use_gpu = len(gpu_ids) > 0\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if use_gpu:\n",
    "        assert(torch.cuda.is_available())\n",
    "\n",
    "    if which_model_netG == 'resnet_9blocks':\n",
    "        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9, gpu_ids=gpu_ids)\n",
    "    else:\n",
    "        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\n",
    "    if len(gpu_ids) > 0:\n",
    "        netG.cuda(gpu_ids[0])\n",
    "    init_weights(netG, init_type=init_type)\n",
    "    return netG\n",
    "\n",
    "\n",
    "def define_D(input_nc, ndf, which_model_netD,\n",
    "             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='normal', gpu_ids=[]):\n",
    "    netD = None\n",
    "    use_gpu = len(gpu_ids) > 0\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if use_gpu:\n",
    "        assert(torch.cuda.is_available())\n",
    "    if which_model_netD == 'basic':\n",
    "        netD = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n",
    "    else:\n",
    "        raise NotImplementedError('Discriminator model name [%s] is not recognized' %\n",
    "                                  which_model_netD)\n",
    "    if use_gpu:\n",
    "        netD.cuda(gpu_ids[0])\n",
    "    init_weights(netD, init_type=init_type)\n",
    "    return netD\n",
    "\n",
    "\n",
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MDqtLWdo7G6t"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Classes\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "# Defines the GAN loss which uses either LSGAN or the regular GAN.\n",
    "# When LSGAN is used, it is basically same as MSELoss,\n",
    "# but it abstracts away the need to create the target label tensor\n",
    "# that has the same size as the input\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n",
    "                 tensor=torch.FloatTensor):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.real_label = target_real_label\n",
    "        self.fake_label = target_fake_label\n",
    "        self.real_label_var = None\n",
    "        self.fake_label_var = None\n",
    "        self.Tensor = tensor\n",
    "        if use_lsgan:\n",
    "            self.loss = nn.MSELoss()\n",
    "        else:\n",
    "            self.loss = nn.BCELoss()\n",
    "\n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        target_tensor = None\n",
    "        if target_is_real:\n",
    "            create_label = ((self.real_label_var is None) or\n",
    "                            (self.real_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n",
    "                self.real_label_var = Variable(real_tensor, requires_grad=False)\n",
    "            target_tensor = self.real_label_var\n",
    "        else:\n",
    "            create_label = ((self.fake_label_var is None) or\n",
    "                            (self.fake_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n",
    "                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n",
    "            target_tensor = self.fake_label_var\n",
    "        return target_tensor\n",
    "\n",
    "    def __call__(self, input, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "        return self.loss(input, target_tensor)\n",
    "\n",
    "\n",
    "# Defines the generator that consists of Resnet blocks between a few\n",
    "# downsampling/upsampling operations.\n",
    "# Code and idea originally from Justin Johnson's architecture.\n",
    "# https://github.com/jcjohnson/fast-neural-style/\n",
    "class ResnetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, gpu_ids=[], padding_type='reflect'):\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "        self.gpu_ids = gpu_ids\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n",
    "                           bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n",
    "                                stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2**n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**(n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "        else:\n",
    "            return self.model(input)\n",
    "\n",
    "\n",
    "# Define a resnet block\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
    "                       norm_layer(dim),\n",
    "                       nn.ReLU(True)]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
    "                       norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Defines the PatchGAN discriminator with the specified arguments.\n",
    "class NLayerDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, gpu_ids=[]):\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        self.gpu_ids = gpu_ids\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2**n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            sequence += [nn.Sigmoid()]\n",
    "\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if len(self.gpu_ids) and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "        else:\n",
    "            return self.model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dujDxdyl76-1"
   },
   "source": [
    "## 2.3 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBBpo8FV79mO"
   },
   "outputs": [],
   "source": [
    "# Converts a Tensor into a Numpy array\n",
    "# |imtype|: the desired type of the converted numpy array\n",
    "def tensor2im(image_tensor, imtype=np.uint8):\n",
    "    image_numpy = image_tensor[0].cpu().float().numpy()\n",
    "    if image_numpy.shape[0] == 1:\n",
    "        image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "\n",
    "def diagnose_network(net, name='network'):\n",
    "    mean = 0.0\n",
    "    count = 0\n",
    "    for param in net.parameters():\n",
    "        if param.grad is not None:\n",
    "            mean += torch.mean(torch.abs(param.grad.data))\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        mean = mean / count\n",
    "    print(name)\n",
    "    print(mean)\n",
    "\n",
    "\n",
    "def save_image(image_numpy, image_path):\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(image_path)\n",
    "\n",
    "\n",
    "def print_numpy(x, val=True, shp=False):\n",
    "    x = x.astype(np.float64)\n",
    "    if shp:\n",
    "        print('shape,', x.shape)\n",
    "    if val:\n",
    "        x = x.flatten()\n",
    "        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n",
    "            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n",
    "\n",
    "\n",
    "def mkdirs(paths):\n",
    "    if isinstance(paths, list) and not isinstance(paths, str):\n",
    "        for path in paths:\n",
    "            mkdir(path)\n",
    "    else:\n",
    "        mkdir(paths)\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvukNsat8Ng-"
   },
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return Variable(images)\n",
    "        return_images = []\n",
    "        for image in images:\n",
    "            image = torch.unsqueeze(image, 0)\n",
    "            if self.num_imgs < self.pool_size:\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:\n",
    "                    random_id = random.randint(0, self.pool_size - 1)\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:\n",
    "                    return_images.append(image)\n",
    "        return_images = Variable(torch.cat(return_images, 0))\n",
    "        return return_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_WEAIh3KzerJ"
   },
   "source": [
    "## 2.4 Cycle GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8_d7pl-zUhM"
   },
   "outputs": [],
   "source": [
    "class CycleGANModel(BaseModel):\n",
    "    def name(self):\n",
    "        return 'CycleGANModel'\n",
    "\n",
    "    def initialize(self, opt):\n",
    "        BaseModel.initialize(self, opt)\n",
    "        # load/define networks\n",
    "        # The naming conversion is different from those used in the paper\n",
    "        # Code (paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n",
    "\n",
    "        self.netG_A = define_G(opt.input_nc, opt.output_nc,\n",
    "                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids)\n",
    "        self.netG_B = define_G(opt.output_nc, opt.input_nc,\n",
    "                                        opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids)\n",
    "\n",
    "        if self.isTrain:\n",
    "            use_sigmoid = opt.no_lsgan\n",
    "            self.netD_A = define_D(opt.output_nc, opt.ndf,\n",
    "                                            opt.which_model_netD,\n",
    "                                            opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, self.gpu_ids)\n",
    "            self.netD_B = define_D(opt.input_nc, opt.ndf,\n",
    "                                            opt.which_model_netD,\n",
    "                                            opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, self.gpu_ids)\n",
    "        if not self.isTrain or opt.continue_train:\n",
    "            which_epoch = opt.which_epoch\n",
    "            self.load_network(self.netG_A, 'G_A', which_epoch)\n",
    "            self.load_network(self.netG_B, 'G_B', which_epoch)\n",
    "            if self.isTrain:\n",
    "                self.load_network(self.netD_A, 'D_A', which_epoch)\n",
    "                self.load_network(self.netD_B, 'D_B', which_epoch)\n",
    "\n",
    "        if self.isTrain:\n",
    "            self.fake_A_pool = ImagePool(opt.pool_size)\n",
    "            self.fake_B_pool = ImagePool(opt.pool_size)\n",
    "            # define loss functions\n",
    "            self.criterionGAN = GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n",
    "            self.criterionCycle = torch.nn.L1Loss()\n",
    "            self.criterionIdt = torch.nn.L1Loss()\n",
    "            # initialize optimizers\n",
    "            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),\n",
    "                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.optimizers = []\n",
    "            self.schedulers = []\n",
    "            self.optimizers.append(self.optimizer_G)\n",
    "            self.optimizers.append(self.optimizer_D_A)\n",
    "            self.optimizers.append(self.optimizer_D_B)\n",
    "            for optimizer in self.optimizers:\n",
    "                self.schedulers.append(get_scheduler(optimizer, opt))\n",
    "\n",
    "        print('---------- Networks initialized -------------')\n",
    "        print_network(self.netG_A)\n",
    "        print_network(self.netG_B)\n",
    "        if self.isTrain:\n",
    "            print_network(self.netD_A)\n",
    "            print_network(self.netD_B)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "    def set_input(self, input):\n",
    "        AtoB = self.opt.which_direction == 'AtoB'\n",
    "        input_A = input['A' if AtoB else 'B']\n",
    "        input_B = input['B' if AtoB else 'A']\n",
    "        if len(self.gpu_ids) > 0:\n",
    "            input_A = input_A.cuda(self.gpu_ids[0], async=True)\n",
    "            input_B = input_B.cuda(self.gpu_ids[0], async=True)\n",
    "        self.input_A = input_A\n",
    "        self.input_B = input_B\n",
    "        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n",
    "\n",
    "    def forward(self):\n",
    "        self.real_A = Variable(self.input_A)\n",
    "        self.real_B = Variable(self.input_B)\n",
    "\n",
    "    def test(self):\n",
    "        real_A = Variable(self.input_A, volatile=True)\n",
    "        fake_B = self.netG_A(real_A)\n",
    "        self.rec_A = self.netG_B(fake_B).data\n",
    "        self.fake_B = fake_B.data\n",
    "\n",
    "        real_B = Variable(self.input_B, volatile=True)\n",
    "        fake_A = self.netG_B(real_B)\n",
    "        self.rec_B = self.netG_A(fake_A).data\n",
    "        self.fake_A = fake_A.data\n",
    "\n",
    "    # get image paths\n",
    "    def get_image_paths(self):\n",
    "        return self.image_paths\n",
    "\n",
    "    def backward_D_basic(self, netD, real, fake):\n",
    "        # Real\n",
    "        pred_real = netD(real)\n",
    "        loss_D_real = self.criterionGAN(pred_real, True)\n",
    "        # Fake\n",
    "        pred_fake = netD(fake.detach())\n",
    "        loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "        # Combined loss\n",
    "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "        # backward\n",
    "        loss_D.backward()\n",
    "        return loss_D\n",
    "\n",
    "    def backward_D_A(self):\n",
    "        fake_B = self.fake_B_pool.query(self.fake_B)\n",
    "        loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n",
    "        self.loss_D_A = loss_D_A.item()\n",
    "\n",
    "    def backward_D_B(self):\n",
    "        fake_A = self.fake_A_pool.query(self.fake_A)\n",
    "        loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n",
    "        self.loss_D_B = loss_D_B.item()\n",
    "\n",
    "    def backward_G(self):\n",
    "        lambda_idt = self.opt.lambda_identity\n",
    "        lambda_A = self.opt.lambda_A\n",
    "        lambda_B = self.opt.lambda_B\n",
    "        # Identity loss\n",
    "        if lambda_idt > 0:\n",
    "            # G_A should be identity if real_B is fed.\n",
    "            idt_A = self.netG_A(self.real_B)\n",
    "            loss_idt_A = self.criterionIdt(idt_A, self.real_B) * lambda_B * lambda_idt\n",
    "            # G_B should be identity if real_A is fed.\n",
    "            idt_B = self.netG_B(self.real_A)\n",
    "            loss_idt_B = self.criterionIdt(idt_B, self.real_A) * lambda_A * lambda_idt\n",
    "\n",
    "            self.idt_A = idt_A.data\n",
    "            self.idt_B = idt_B.data\n",
    "            self.loss_idt_A = loss_idt_A.item()\n",
    "            self.loss_idt_B = loss_idt_B.item()\n",
    "        else:\n",
    "            loss_idt_A = 0\n",
    "            loss_idt_B = 0\n",
    "            self.loss_idt_A = 0\n",
    "            self.loss_idt_B = 0\n",
    "\n",
    "        # GAN loss D_A(G_A(A))\n",
    "        fake_B = self.netG_A(self.real_A)\n",
    "        pred_fake = self.netD_A(fake_B)\n",
    "        loss_G_A = self.criterionGAN(pred_fake, True)\n",
    "\n",
    "        # GAN loss D_B(G_B(B))\n",
    "        fake_A = self.netG_B(self.real_B)\n",
    "        pred_fake = self.netD_B(fake_A)\n",
    "        loss_G_B = self.criterionGAN(pred_fake, True)\n",
    "\n",
    "        # Forward cycle loss\n",
    "        rec_A = self.netG_B(fake_B)\n",
    "        loss_cycle_A = self.criterionCycle(rec_A, self.real_A) * lambda_A\n",
    "\n",
    "        # Backward cycle loss\n",
    "        rec_B = self.netG_A(fake_A)\n",
    "        loss_cycle_B = self.criterionCycle(rec_B, self.real_B) * lambda_B\n",
    "        # combined loss\n",
    "        loss_G = loss_G_A + loss_G_B + loss_cycle_A + loss_cycle_B + loss_idt_A + loss_idt_B\n",
    "        loss_G.backward()\n",
    "\n",
    "        self.fake_B = fake_B.data\n",
    "        self.fake_A = fake_A.data\n",
    "        self.rec_A = rec_A.data\n",
    "        self.rec_B = rec_B.data\n",
    "\n",
    "        self.loss_G_A = loss_G_A.item()\n",
    "        self.loss_G_B = loss_G_B.item()\n",
    "        self.loss_cycle_A = loss_cycle_A.item()\n",
    "        self.loss_cycle_B = loss_cycle_B.item()\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        # forward\n",
    "        self.forward()\n",
    "        # G_A and G_B\n",
    "        self.optimizer_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.optimizer_G.step()\n",
    "        # D_A\n",
    "        self.optimizer_D_A.zero_grad()\n",
    "        self.backward_D_A()\n",
    "        self.optimizer_D_A.step()\n",
    "        # D_B\n",
    "        self.optimizer_D_B.zero_grad()\n",
    "        self.backward_D_B()\n",
    "        self.optimizer_D_B.step()\n",
    "\n",
    "    def get_current_errors(self):\n",
    "        ret_errors = OrderedDict([('D_A', self.loss_D_A), ('G_A', self.loss_G_A), ('Cyc_A', self.loss_cycle_A),\n",
    "                                  ('D_B', self.loss_D_B), ('G_B', self.loss_G_B), ('Cyc_B', self.loss_cycle_B)])\n",
    "        if self.opt.lambda_identity > 0.0:\n",
    "            ret_errors['idt_A'] = self.loss_idt_A\n",
    "            ret_errors['idt_B'] = self.loss_idt_B\n",
    "        return ret_errors\n",
    "\n",
    "    def get_current_visuals(self):\n",
    "        real_A = tensor2im(self.input_A)\n",
    "        fake_B = tensor2im(self.fake_B)\n",
    "        rec_A = tensor2im(self.rec_A)\n",
    "        real_B = tensor2im(self.input_B)\n",
    "        fake_A = tensor2im(self.fake_A)\n",
    "        rec_B = tensor2im(self.rec_B)\n",
    "        ret_visuals = OrderedDict([('real_A', real_A), ('fake_B', fake_B), ('rec_A', rec_A),\n",
    "                                   ('real_B', real_B), ('fake_A', fake_A), ('rec_B', rec_B)])\n",
    "        if self.opt.isTrain and self.opt.lambda_identity > 0.0:\n",
    "            ret_visuals['idt_A'] = tensor2im(self.idt_A)\n",
    "            ret_visuals['idt_B'] = tensor2im(self.idt_B)\n",
    "        return ret_visuals\n",
    "\n",
    "    def save(self, label):\n",
    "        self.save_network(self.netG_A, 'G_A', label, self.gpu_ids)\n",
    "        self.save_network(self.netD_A, 'D_A', label, self.gpu_ids)\n",
    "        self.save_network(self.netG_B, 'G_B', label, self.gpu_ids)\n",
    "        self.save_network(self.netD_B, 'D_B', label, self.gpu_ids)\n",
    "\n",
    "    def load(self, label):\n",
    "        self.load_network(self.netG_A, 'G_A', label)\n",
    "        self.load_network(self.netD_A, 'D_A', label)\n",
    "        self.load_network(self.netG_B, 'G_B', label)\n",
    "        self.load_network(self.netD_B, 'D_B', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDSI841_zPWi"
   },
   "outputs": [],
   "source": [
    "def create_model(opt):\n",
    "    model = None\n",
    "    print(opt.model)\n",
    "    if opt.model == 'cycle_gan':\n",
    "        assert(opt.dataset_mode == 'unaligned')\n",
    "        model = CycleGANModel()\n",
    "    else:\n",
    "        raise ValueError(\"Model [%s] not recognized.\" % opt.model)\n",
    "    model.initialize(opt)\n",
    "    print(\"model [%s] was created\" % (model.name()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HJ80i93yY2J"
   },
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urkNjHnaGKU0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mCDZt7o9_iYw",
    "outputId": "9f8ef46e-1e25-405f-e12a-06f012f542e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cycle_gan\n",
      "initialization method [normal]\n",
      "initialization method [normal]\n",
      "initialization method [normal]\n",
      "initialization method [normal]\n",
      "---------- Networks initialized -------------\n",
      "ResnetGenerator(\n",
      "  (model): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (11): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (12): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (13): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (14): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (15): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (16): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (17): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (18): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (19): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (20): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (21): ReLU(inplace=True)\n",
      "    (22): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (23): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (26): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (27): Tanh()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 11378179\n",
      "ResnetGenerator(\n",
      "  (model): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (11): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (12): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (13): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (14): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (15): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (16): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (17): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (18): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (19): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (20): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (21): ReLU(inplace=True)\n",
      "    (22): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (23): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (26): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (27): Tanh()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 11378179\n",
      "NLayerDiscriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 2764737\n",
      "NLayerDiscriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 2764737\n",
      "-----------------------------------------------\n",
      "model [CycleGANModel] was created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "model = create_model(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "J-rht9c-n_JU",
    "outputId": "3f318693-376e-418a-e97e-42f3633ae9ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of cuda is deprecated:\n",
      "\tcuda(torch.device device, bool async, *, torch.memory_format memory_format)\n",
      "Consider using one of the following signatures instead:\n",
      "\tcuda(torch.device device, bool non_blocking, *, torch.memory_format memory_format)\n",
      "1334it [35:03,  1.58s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1 / 200 \t Time Taken: 2103 sec\n",
      "learning rate = 0.0002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1334it [35:06,  1.58s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 2 / 200 \t Time Taken: 2106 sec\n",
      "learning rate = 0.0002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1334it [35:08,  1.58s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 3 / 200 \t Time Taken: 2108 sec\n",
      "learning rate = 0.0002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [03:17,  1.58s/it]"
     ]
    }
   ],
   "source": [
    "total_steps = 0\n",
    "\n",
    "for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    iter_data_time = time.time()\n",
    "    epoch_iter = 0\n",
    "\n",
    "    for i, data in tqdm(enumerate(dataset)):\n",
    "        iter_start_time = time.time()\n",
    "        if total_steps % opt.print_freq == 0: t_data = iter_start_time - iter_data_time\n",
    "        total_steps += opt.batchSize\n",
    "        epoch_iter += opt.batchSize\n",
    "        model.set_input(data)\n",
    "        model.optimize_parameters()\n",
    "\n",
    "        if total_steps % opt.print_freq == 0:\n",
    "            errors = model.get_current_errors()\n",
    "            t = (time.time() - iter_start_time) / opt.batchSize\n",
    "\n",
    "        if total_steps % opt.save_latest_freq == 0:\n",
    "            print('saving the latest model (epoch %d, total_steps %d)' % (epoch, total_steps))\n",
    "            model.save('latest')\n",
    "\n",
    "        iter_data_time = time.time()\n",
    "        \n",
    "    if epoch % opt.save_epoch_freq == 0:\n",
    "        print('saving the model at the end of epoch %d, iters %d' % (epoch, total_steps))\n",
    "        model.save('latest')\n",
    "        model.save(epoch)\n",
    "\n",
    "    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n",
    "          (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n",
    "    model.update_learning_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4grbzi3vcKE"
   },
   "outputs": [],
   "source": [
    "model.save('latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtxavWLnybl1"
   },
   "source": [
    "# 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAn7lVOjGHnU"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qu8jUtTbGBGN"
   },
   "outputs": [],
   "source": [
    "model.load('latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KxbIgswIn_Jd"
   },
   "outputs": [],
   "source": [
    "def show_img(im, ax=None, figsize=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtEneOCPn_Jm"
   },
   "outputs": [],
   "source": [
    "def get_one(data):\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    return list(model.get_current_visuals().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPlnmvq8n_J2"
   },
   "outputs": [],
   "source": [
    "test_ims = []\n",
    "for i,o in enumerate(dataset):\n",
    "    if i>10: break\n",
    "    test_ims.append(get_one(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44hKlac5n_J9"
   },
   "outputs": [],
   "source": [
    "def show_grid(ims):\n",
    "    fig,axes = plt.subplots(2,3,figsize=(9,6))\n",
    "    for i,ax in enumerate(axes.flat): show_img(ims[i], ax);\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "klpmo83Zn_KD"
   },
   "outputs": [],
   "source": [
    "for i in range(8): show_grid(test_ims[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Rs6OMsiHHJQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "6oxbRqPuyVpp",
    "3zr4OBnQzcCs",
    "dujDxdyl76-1",
    "_WEAIh3KzerJ"
   ],
   "name": "CycleGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
